from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import pandas as pd
import uuid
import logging
from typing import List, Dict, Any, Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI(title="DaPlot API")

# Add CORS middleware to allow cross-origin requests
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods
    allow_headers=["*"],  # Allows all headers
)

# A simple in-memory storage for uploaded dataframes and file metadata
data_storage = {}
file_metadata = {}  # å­˜å‚¨æ–‡ä»¶å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬åŸå§‹æ–‡ä»¶åå’Œsheetä¿¡æ¯

class FilterPayload(BaseModel):
    file_id: str
    filters: Dict[str, List[str]]

class PlotDataPayload(BaseModel):
    file_id: str
    filters: Dict[str, List[str]]
    x_axis: str
    y_axis: str

class SaveFilePayload(BaseModel):
    file_id: str
    headers: List[str]
    data: List[List[Any]]
    filename: Optional[str] = None

class FileInfo(BaseModel):
    file_id: str
    filename: str
    sheet_name: Optional[str] = None
    rows: int
    columns: int
    headers: List[str]

@app.get("/")
def read_root():
    return {"message": "Welcome to DaPlot API"}

@app.post("/api/upload")
async def upload_excel_file(file: UploadFile = File(...)):
    """
    Handles the upload of an Excel file, processes it, and returns a preview.
    Supports multiple sheets and returns information about all sheets.
    """
    logger.info(f"ğŸ“ æ”¶åˆ°æ–‡ä»¶ä¸Šä¼ è¯·æ±‚: {file.filename}")
    logger.info(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file.size if hasattr(file, 'size') else 'æœªçŸ¥'} bytes")
    logger.info(f"ğŸ“‹ æ–‡ä»¶ç±»å‹: {file.content_type}")
    
    # Check if the file is an Excel file
    if not file.filename.endswith(('.xlsx', '.xls')):
        logger.error(f"âŒ æ— æ•ˆæ–‡ä»¶ç±»å‹: {file.filename}")
        raise HTTPException(status_code=400, detail="Invalid file type. Please upload an Excel file.")

    try:
        logger.info("ğŸ”„ å¼€å§‹è¯»å–Excelæ–‡ä»¶...")
        
        # é¦–å…ˆè¯»å–æ‰€æœ‰sheetåç§°
        excel_file = pd.ExcelFile(file.file)
        sheet_names = excel_file.sheet_names
        logger.info(f"ğŸ“‹ å‘ç° {len(sheet_names)} ä¸ªå·¥ä½œè¡¨: {sheet_names}")
        
        uploaded_files = []
        
        # ä¸ºæ¯ä¸ªsheetåˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„æ–‡ä»¶è®°å½•
        for sheet_name in sheet_names:
            try:
                # è¯»å–ç‰¹å®šsheetçš„æ•°æ®
                df = pd.read_excel(excel_file, sheet_name=sheet_name)
                logger.info(f"âœ… å·¥ä½œè¡¨ '{sheet_name}' è¯»å–æˆåŠŸ! æ•°æ®å½¢çŠ¶: {df.shape}")
                logger.info(f"ğŸ“Š åˆ—å: {df.columns.tolist()}")
                
                # ä¸ºæ¯ä¸ªsheetç”Ÿæˆå”¯ä¸€ID
                file_id = str(uuid.uuid4())
                logger.info(f"ğŸ†” ä¸ºå·¥ä½œè¡¨ '{sheet_name}' ç”Ÿæˆæ–‡ä»¶ID: {file_id}")
                
                # å­˜å‚¨æ•°æ®å’Œå…ƒæ•°æ®
                data_storage[file_id] = df
                file_metadata[file_id] = {
                    "original_filename": file.filename,
                    "sheet_name": sheet_name,
                    "upload_time": pd.Timestamp.now().isoformat()
                }
                
                # è·å–è¡¨å¤´
                headers = df.columns.tolist()
                
                # è·å–é¢„è§ˆæ•°æ®ï¼ˆå‰5è¡Œï¼‰
                preview_df = df.head()
                preview_data = preview_df.where(pd.notnull(preview_df), None).to_dict(orient='records')
                
                # æ„å»ºæ–‡ä»¶ä¿¡æ¯
                file_info = {
                    "file_id": file_id,
                    "filename": f"{file.filename} - {sheet_name}" if len(sheet_names) > 1 else file.filename,
                    "original_filename": file.filename,
                    "sheet_name": sheet_name,
                    "headers": headers,
                    "preview_data": preview_data,
                    "rows": len(df),
                    "columns": len(headers)
                }
                
                uploaded_files.append(file_info)
                logger.info(f"âœ… å·¥ä½œè¡¨ '{sheet_name}' å¤„ç†å®Œæˆ")
                
            except Exception as sheet_error:
                logger.error(f"âŒ å¤„ç†å·¥ä½œè¡¨ '{sheet_name}' æ—¶å‡ºé”™: {str(sheet_error)}")
                continue
        
        if not uploaded_files:
            raise HTTPException(status_code=400, detail="No valid sheets found in the Excel file")
        
        logger.info(f"ğŸ’¾ æ•°æ®å·²å­˜å‚¨åˆ°å†…å­˜ï¼Œå½“å‰å­˜å‚¨çš„æ–‡ä»¶æ•°é‡: {len(data_storage)}")
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªsheetï¼Œè¿”å›å•ä¸ªæ–‡ä»¶æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
        if len(uploaded_files) == 1:
            response_data = uploaded_files[0]
        else:
            # å¤šä¸ªsheetæ—¶è¿”å›æ–‡ä»¶åˆ—è¡¨
            response_data = {
                "multiple_sheets": True,
                "files": uploaded_files,
                "total_sheets": len(uploaded_files)
            }
        
        logger.info(f"âœ… æ–‡ä»¶ä¸Šä¼ å¤„ç†å®Œæˆ: {file.filename}")
        return response_data
        
    except Exception as e:
        logger.error(f"âŒ Excelæ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=400, detail=f"Error processing Excel file: {e}")

@app.post("/api/filter")
async def filter_data(payload: FilterPayload):
    """
    Filters the dataframe based on the provided criteria.
    """
    logger.info(f"ğŸ” [åç«¯] å¼€å§‹æ•°æ®ç­›é€‰ï¼Œæ–‡ä»¶ID: {payload.file_id}")
    logger.info(f"ğŸ” [åç«¯] ç­›é€‰æ¡ä»¶: {payload.filters}")
    
    df = data_storage.get(payload.file_id)
    if df is None:
        raise HTTPException(status_code=404, detail="File ID not found.")

    logger.info(f"ğŸ“Š [åç«¯] åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
    logger.info(f"ğŸ“Š [åç«¯] æ•°æ®åˆ—å: {df.columns.tolist()}")
    
    filtered_df = df.copy()

    for column, values in payload.filters.items():
        if column in filtered_df.columns:
            if values: # Ensure there are values to filter by
                logger.info(f"ğŸ” [åç«¯] ç­›é€‰åˆ— '{column}', ç­›é€‰å€¼: {values} (ç±»å‹: {[type(v).__name__ for v in values]})")
                
                # æ£€æŸ¥æ•°æ®åˆ—çš„å®é™…æ•°æ®ç±»å‹
                sample_data = filtered_df[column].dropna().head(5).tolist()
                logger.info(f"ğŸ“Š [åç«¯] åˆ— '{column}' æ ·æœ¬æ•°æ®: {sample_data} (ç±»å‹: {[type(v).__name__ for v in sample_data]})")
                
                # å°è¯•æ•°æ®ç±»å‹è½¬æ¢åŒ¹é…
                original_count = len(filtered_df)
                
                # æ–¹æ³•1: ç›´æ¥åŒ¹é…
                mask1 = filtered_df[column].isin(values)
                count1 = mask1.sum()
                
                # æ–¹æ³•2: è½¬æ¢ä¸ºå­—ç¬¦ä¸²ååŒ¹é…
                str_values = [str(v) for v in values]
                mask2 = filtered_df[column].astype(str).isin(str_values)
                count2 = mask2.sum()
                
                # æ–¹æ³•3: å°è¯•å°†æ•°æ®åˆ—è½¬æ¢ä¸ºæ•°å­—ååŒ¹é…
                try:
                    numeric_column = pd.to_numeric(filtered_df[column], errors='coerce')
                    numeric_values = []
                    for v in values:
                        try:
                            numeric_values.append(float(v))
                        except (ValueError, TypeError):
                            numeric_values.append(v)
                    mask3 = numeric_column.isin(numeric_values)
                    count3 = mask3.sum()
                except:
                    count3 = 0
                    mask3 = pd.Series([False] * len(filtered_df))
                
                logger.info(f"ğŸ” [åç«¯] åŒ¹é…ç»“æœ - ç›´æ¥åŒ¹é…: {count1}, å­—ç¬¦ä¸²åŒ¹é…: {count2}, æ•°å­—åŒ¹é…: {count3}")
                
                # é€‰æ‹©åŒ¹é…æ•°é‡æœ€å¤šçš„æ–¹æ³•
                if count3 > 0 and count3 >= max(count1, count2):
                    filtered_df = filtered_df[mask3]
                    logger.info(f"âœ… [åç«¯] ä½¿ç”¨æ•°å­—åŒ¹é…ï¼Œç­›é€‰åæ•°æ®è¡Œæ•°: {len(filtered_df)}")
                elif count2 > 0 and count2 >= count1:
                    filtered_df = filtered_df[mask2]
                    logger.info(f"âœ… [åç«¯] ä½¿ç”¨å­—ç¬¦ä¸²åŒ¹é…ï¼Œç­›é€‰åæ•°æ®è¡Œæ•°: {len(filtered_df)}")
                else:
                    filtered_df = filtered_df[mask1]
                    logger.info(f"âœ… [åç«¯] ä½¿ç”¨ç›´æ¥åŒ¹é…ï¼Œç­›é€‰åæ•°æ®è¡Œæ•°: {len(filtered_df)}")
                    
        else:
            # Optionally, raise an error if the column doesn't exist
            logger.error(f"âŒ [åç«¯] ç­›é€‰åˆ— '{column}' åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨")
            raise HTTPException(status_code=400, detail=f"Filter column '{column}' not found in data.")

    logger.info(f"âœ… [åç«¯] æ•°æ®ç­›é€‰å®Œæˆï¼Œæœ€ç»ˆæ•°æ®è¡Œæ•°: {len(filtered_df)}")
    
    # Convert NaN to None for JSON compatibility and return as records
    result = filtered_df.where(pd.notnull(filtered_df), None).to_dict(orient='records')
    logger.info(f"ğŸ“¤ [åç«¯] è¿”å›ç­›é€‰ç»“æœ: {len(result)} è¡Œæ•°æ®")
    
    return result

@app.get("/api/file/{file_id}")
async def get_file_data(file_id: str):
    """
    Retrieves the complete data for a specific file ID.
    """
    logger.info(f"ğŸ“ è¯·æ±‚è·å–æ–‡ä»¶æ•°æ®: {file_id}")
    
    df = data_storage.get(file_id)
    if df is None:
        logger.error(f"âŒ æ–‡ä»¶IDæœªæ‰¾åˆ°: {file_id}")
        raise HTTPException(status_code=404, detail="File ID not found.")
    
    # Get headers
    headers = df.columns.tolist()
    
    # Get all data (convert NaN to None for JSON compatibility)
    all_data = df.where(pd.notnull(df), None).to_dict(orient='records')
    
    logger.info(f"âœ… æ–‡ä»¶æ•°æ®è·å–æˆåŠŸ: {len(all_data)}è¡Œ Ã— {len(headers)}åˆ—")
    
    return {
        "file_id": file_id,
        "filename": f"file_{file_id[:8]}.xlsx",  # Generate a filename since we don't store original names
        "headers": headers,
        "preview_data": all_data  # Return all data for editing
    }

@app.post("/api/plot_data")
async def get_plot_data(payload: PlotDataPayload):
    """
    Prepares data for plotting by filtering and extracting x and y axis values.
    """
    df = data_storage.get(payload.file_id)
    if df is None:
        raise HTTPException(status_code=404, detail="File ID not found.")

    # Apply filters first
    filtered_df = df.copy()
    for column, values in payload.filters.items():
        if column in filtered_df.columns:
            if values:  # Ensure there are values to filter by
                filtered_df = filtered_df[filtered_df[column].isin(values)]
        else:
            raise HTTPException(status_code=400, detail=f"Filter column '{column}' not found in data.")

    # Check if x_axis and y_axis columns exist
    if payload.x_axis not in filtered_df.columns:
        raise HTTPException(status_code=400, detail=f"X-axis column '{payload.x_axis}' not found in data.")
    if payload.y_axis not in filtered_df.columns:
        raise HTTPException(status_code=400, detail=f"Y-axis column '{payload.y_axis}' not found in data.")

    # Extract x and y values, removing any NaN values
    x_values = filtered_df[payload.x_axis].dropna().tolist()
    y_values = filtered_df[payload.y_axis].dropna().tolist()

    # Ensure both lists have the same length by taking the minimum length
    min_length = min(len(x_values), len(y_values))
    x_values = x_values[:min_length]
    y_values = y_values[:min_length]

    return {
        "x_values": x_values,
        "y_values": y_values,
        "x_label": payload.x_axis,
        "y_label": payload.y_axis
    }

@app.post("/api/save")
async def save_file_data(payload: SaveFilePayload):
    """
    Saves updated file data back to storage.
    """
    logger.info(f"ğŸ“ è¯·æ±‚ä¿å­˜æ–‡ä»¶æ•°æ®: {payload.file_id}")
    
    try:
        # éªŒè¯æ•°æ®æ ¼å¼
        if not payload.headers or not isinstance(payload.headers, list):
            raise HTTPException(status_code=400, detail="Invalid headers format")
        
        if not isinstance(payload.data, list):
            raise HTTPException(status_code=400, detail="Invalid data format")
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(payload.data, columns=payload.headers)
        
        # æ›´æ–°å­˜å‚¨
        data_storage[payload.file_id] = df
        
        logger.info(f"âœ… æ–‡ä»¶æ•°æ®ä¿å­˜æˆåŠŸ: {payload.file_id}, æ•°æ®å½¢çŠ¶: {df.shape}")
        
        return {
            "success": True,
            "message": "File data saved successfully",
            "file_id": payload.file_id,
            "rows": len(payload.data),
            "columns": len(payload.headers)
        }
        
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜æ–‡ä»¶æ•°æ®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error saving file data: {e}")

@app.get("/api/files")
def list_files():
    """
    Returns a list of all stored files with their metadata.
    """
    logger.info(f"ğŸ“‹ è·å–æ–‡ä»¶åˆ—è¡¨è¯·æ±‚ï¼Œå½“å‰å­˜å‚¨æ–‡ä»¶æ•°: {len(data_storage)}")
    
    files_info = []
    for file_id, df in data_storage.items():
        # è·å–æ–‡ä»¶å…ƒæ•°æ®
        metadata = file_metadata.get(file_id, {})
        original_filename = metadata.get("original_filename", f"file_{file_id[:8]}.xlsx")
        sheet_name = metadata.get("sheet_name")
        
        # æ„å»ºæ˜¾ç¤ºæ–‡ä»¶å
        if sheet_name:
            display_filename = f"{original_filename} - {sheet_name}"
        else:
            display_filename = original_filename
        
        file_info = {
            "file_id": file_id,
            "filename": display_filename,
            "original_filename": original_filename,
            "sheet_name": sheet_name,
            "rows": len(df),
            "columns": len(df.columns),
            "headers": df.columns.tolist(),
            "upload_time": metadata.get("upload_time")
        }
        files_info.append(file_info)
    
    logger.info(f"âœ… è¿”å› {len(files_info)} ä¸ªæ–‡ä»¶çš„ä¿¡æ¯")
    return {"files": files_info}

@app.delete("/api/file/{file_id}")
async def delete_file_data(file_id: str):
    """
    Deletes a file from storage.
    """
    logger.info(f"ğŸ—‘ï¸ è¯·æ±‚åˆ é™¤æ–‡ä»¶: {file_id}")
    
    if file_id not in data_storage:
        logger.error(f"âŒ æ–‡ä»¶IDæœªæ‰¾åˆ°: {file_id}")
        raise HTTPException(status_code=404, detail="File ID not found.")
    
    del data_storage[file_id]
    logger.info(f"âœ… æ–‡ä»¶åˆ é™¤æˆåŠŸ: {file_id}")
    
    return {
        "success": True,
        "message": "File deleted successfully",
        "file_id": file_id
    }

@app.get("/api/unique_values/{file_id}/{column_name}")
async def get_unique_values(file_id: str, column_name: str):
    """
    Returns unique values for a specific column in a file.
    """
    logger.info(f"ğŸ” è¯·æ±‚è·å–å”¯ä¸€å€¼: æ–‡ä»¶ID={file_id}, åˆ—å={column_name}")
    
    df = data_storage.get(file_id)
    if df is None:
        logger.error(f"âŒ æ–‡ä»¶IDæœªæ‰¾åˆ°: {file_id}")
        raise HTTPException(status_code=404, detail="File ID not found.")
    
    if column_name not in df.columns:
        logger.error(f"âŒ åˆ—åæœªæ‰¾åˆ°: {column_name}")
        raise HTTPException(status_code=404, detail=f"Column '{column_name}' not found in data.")
    
    try:
        # è·å–å”¯ä¸€å€¼ï¼Œæ’é™¤NaN
        unique_values = df[column_name].dropna().unique().tolist()
        logger.info(f"âœ… è·å–åˆ° {len(unique_values)} ä¸ªå”¯ä¸€å€¼")
        
        return {
            "values": unique_values,
            "count": len(unique_values),
            "column": column_name
        }
        
    except Exception as e:
        logger.error(f"âŒ è·å–å”¯ä¸€å€¼å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting unique values: {e}")
