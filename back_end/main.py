from fastapi import FastAPI, File, UploadFile, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import pandas as pd
import numpy as np
import uuid
import logging
from typing import List, Dict, Any, Optional
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI(title="DaPlot API")

# Add CORS middleware to allow cross-origin requests
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods
    allow_headers=["*"],  # Allows all headers
)

# A simple in-memory storage for uploaded dataframes and file metadata
data_storage = {}
file_metadata = {}  # å­˜å‚¨æ–‡ä»¶å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬åŸå§‹æ–‡ä»¶åå’Œsheetä¿¡æ¯

class FilterPayload(BaseModel):
    file_id: str
    filters: Dict[str, List[str]]

class PlotDataPayload(BaseModel):
    file_id: str
    filters: Dict[str, List[str]]
    x_axis: str
    y_axis: str

class SaveFilePayload(BaseModel):
    file_id: str
    headers: List[str]
    data: List[List[Any]]
    filename: Optional[str] = None

class FileInfo(BaseModel):
    file_id: str
    filename: str
    sheet_name: Optional[str] = None
    rows: int
    columns: int
    headers: List[str]

class PredictionPayload(BaseModel):
    file_id: str
    filters: Dict[str, List[str]]
    x_axis: str
    y_axis: str
    method: str
    steps: int = 10

class PredictionResult(BaseModel):
    x_values: List[float]
    y_values: List[float]
    method: str
    steps: int
    metrics: Dict[str, float]
    model_info: Dict[str, Any]

class DirectPredictionPayload(BaseModel):
    x_values: List[float]
    y_values: List[float]
    method: str
    steps: int = 10

@app.get("/")
def read_root():
    return {"message": "Welcome to DaPlot API"}

@app.post("/api/upload-debug")
async def upload_debug(request: Request):
    """è°ƒè¯•ä¸Šä¼ è¯·æ±‚"""
    logger.info("ğŸ” æ”¶åˆ°è°ƒè¯•ä¸Šä¼ è¯·æ±‚")
    
    # è·å–è¯·æ±‚å¤´
    headers = dict(request.headers)
    logger.info(f"ğŸ“‹ è¯·æ±‚å¤´: {headers}")
    
    # è·å–Content-Type
    content_type = request.headers.get("content-type", "")
    logger.info(f"ğŸ“‹ Content-Type: {content_type}")
    
    # å°è¯•è¯»å–åŸå§‹è¯·æ±‚ä½“
    try:
        body = await request.body()
        logger.info(f"ğŸ“¦ è¯·æ±‚ä½“å¤§å°: {len(body)} bytes")
        logger.info(f"ğŸ“¦ è¯·æ±‚ä½“å‰100å­—èŠ‚: {body[:100]}")
    except Exception as e:
        logger.error(f"âŒ è¯»å–è¯·æ±‚ä½“å¤±è´¥: {e}")
    
    return {"status": "debug", "content_type": content_type, "body_size": len(body) if 'body' in locals() else 0}

@app.post("/api/upload-simple")
async def upload_simple(file: UploadFile):
    """ç®€åŒ–çš„æ–‡ä»¶ä¸Šä¼ ç«¯ç‚¹"""
    logger.info("ğŸ” æ”¶åˆ°ç®€åŒ–ä¸Šä¼ è¯·æ±‚")
    logger.info(f"ğŸ“ æ–‡ä»¶å: {file.filename}")
    logger.info(f"ğŸ“‹ æ–‡ä»¶ç±»å‹: {file.content_type}")
    
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        logger.info(f"ğŸ“¦ æ–‡ä»¶å¤§å°: {len(content)} bytes")
        
        return {
            "filename": file.filename,
            "content_type": file.content_type,
            "size": len(content),
            "status": "success"
        }
    except Exception as e:
        logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/upload")
async def upload_excel_file(file: UploadFile = File(...)):
    """
    Handles the upload of an Excel file, processes it, and returns a preview.
    Supports multiple sheets and returns information about all sheets.
    """
    try:
        logger.info(f"ğŸ“ æ”¶åˆ°æ–‡ä»¶ä¸Šä¼ è¯·æ±‚")
        logger.info(f"ğŸ“ æ–‡ä»¶å: {file.filename}")
        logger.info(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file.size if hasattr(file, 'size') else 'æœªçŸ¥'} bytes")
        logger.info(f"ğŸ“‹ æ–‡ä»¶ç±»å‹: {file.content_type}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©º
        if not file.filename:
            logger.error("âŒ æ–‡ä»¶åä¸ºç©º")
            raise HTTPException(status_code=400, detail="No file provided or filename is empty.")
        
        # Check if the file is an Excel file
        if not file.filename.endswith(('.xlsx', '.xls')):
            logger.error(f"âŒ æ— æ•ˆæ–‡ä»¶ç±»å‹: {file.filename}")
            raise HTTPException(status_code=400, detail="Invalid file type. Please upload an Excel file.")
            
    except Exception as validation_error:
        logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ éªŒè¯å¤±è´¥: {str(validation_error)}")
        logger.error(f"âŒ é”™è¯¯ç±»å‹: {type(validation_error).__name__}")
        raise HTTPException(status_code=422, detail=f"File validation failed: {str(validation_error)}")

    try:
        logger.info("ğŸ”„ å¼€å§‹è¯»å–Excelæ–‡ä»¶...")

        # é¦–å…ˆè¯»å–æ‰€æœ‰sheetåç§°
        excel_file = pd.ExcelFile(file.file)
        sheet_names = excel_file.sheet_names
        logger.info(f"ğŸ“‹ å‘ç° {len(sheet_names)} ä¸ªå·¥ä½œè¡¨: {sheet_names}")

        uploaded_files = []

        # ä¸ºæ¯ä¸ªsheetåˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„æ–‡ä»¶è®°å½•
        for sheet_name in sheet_names:
            try:
                # è¯»å–ç‰¹å®šsheetçš„æ•°æ®
                df = pd.read_excel(excel_file, sheet_name=sheet_name)
                logger.info(f"âœ… å·¥ä½œè¡¨ '{sheet_name}' è¯»å–æˆåŠŸ! æ•°æ®å½¢çŠ¶: {df.shape}")
                logger.info(f"ğŸ“Š åˆ—å: {df.columns.tolist()}")

                # ä¸ºæ¯ä¸ªsheetç”Ÿæˆå”¯ä¸€ID
                file_id = str(uuid.uuid4())
                logger.info(f"ğŸ†” ä¸ºå·¥ä½œè¡¨ '{sheet_name}' ç”Ÿæˆæ–‡ä»¶ID: {file_id}")

                # å­˜å‚¨æ•°æ®å’Œå…ƒæ•°æ®
                data_storage[file_id] = df
                file_metadata[file_id] = {
                    "original_filename": file.filename,
                    "sheet_name": sheet_name,
                    "upload_time": pd.Timestamp.now().isoformat()
                }

                # è·å–è¡¨å¤´
                headers = df.columns.tolist()

                # è·å–é¢„è§ˆæ•°æ®ï¼ˆå‰5è¡Œï¼‰
                preview_df = df.head()
                preview_data = preview_df.where(pd.notnull(preview_df), None).to_dict(orient='records')

                # æ„å»ºæ–‡ä»¶ä¿¡æ¯
                file_info = {
                    "file_id": file_id,
                    "filename": f"{file.filename} - {sheet_name}" if len(sheet_names) > 1 else file.filename,
                    "original_filename": file.filename,
                    "sheet_name": sheet_name,
                    "headers": headers,
                    "preview_data": preview_data,
                    "rows": len(df),
                    "columns": len(headers)
                }

                uploaded_files.append(file_info)
                logger.info(f"âœ… å·¥ä½œè¡¨ '{sheet_name}' å¤„ç†å®Œæˆ")

            except Exception as sheet_error:
                logger.error(f"âŒ å¤„ç†å·¥ä½œè¡¨ '{sheet_name}' æ—¶å‡ºé”™: {str(sheet_error)}")
                continue

        if not uploaded_files:
            raise HTTPException(status_code=400, detail="No valid sheets found in the Excel file")

        logger.info(f"ğŸ’¾ æ•°æ®å·²å­˜å‚¨åˆ°å†…å­˜ï¼Œå½“å‰å­˜å‚¨çš„æ–‡ä»¶æ•°é‡: {len(data_storage)}")

        # å¦‚æœåªæœ‰ä¸€ä¸ªsheetï¼Œè¿”å›å•ä¸ªæ–‡ä»¶æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
        if len(uploaded_files) == 1:
            response_data = uploaded_files[0]
        else:
            # å¤šä¸ªsheetæ—¶è¿”å›æ–‡ä»¶åˆ—è¡¨
            response_data = {
                "multiple_sheets": True,
                "files": uploaded_files,
                "total_sheets": len(uploaded_files)
            }

        logger.info(f"âœ… æ–‡ä»¶ä¸Šä¼ å¤„ç†å®Œæˆ: {file.filename}")
        return response_data

    except Exception as e:
        logger.error(f"âŒ Excelæ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=400, detail=f"Error processing Excel file: {e}")

@app.post("/api/filter")
async def filter_data(payload: FilterPayload):
    """
    Filters the dataframe based on the provided criteria.
    """
    logger.info(f"ğŸ” [åç«¯] å¼€å§‹æ•°æ®ç­›é€‰ï¼Œæ–‡ä»¶ID: {payload.file_id}")
    logger.info(f"ğŸ” [åç«¯] ç­›é€‰æ¡ä»¶: {payload.filters}")

    df = data_storage.get(payload.file_id)
    if df is None:
        raise HTTPException(status_code=404, detail="File ID not found.")

    logger.info(f"ğŸ“Š [åç«¯] åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
    logger.info(f"ğŸ“Š [åç«¯] æ•°æ®åˆ—å: {df.columns.tolist()}")

    filtered_df = df.copy()

    for column, values in payload.filters.items():
        if column in filtered_df.columns:
            if values: # Ensure there are values to filter by
                logger.info(f"ğŸ” [åç«¯] ç­›é€‰åˆ— '{column}', ç­›é€‰å€¼: {values} (ç±»å‹: {[type(v).__name__ for v in values]})")

                # æ£€æŸ¥æ•°æ®åˆ—çš„å®é™…æ•°æ®ç±»å‹
                sample_data = filtered_df[column].dropna().head(5).tolist()
                logger.info(f"ğŸ“Š [åç«¯] åˆ— '{column}' æ ·æœ¬æ•°æ®: {sample_data} (ç±»å‹: {[type(v).__name__ for v in sample_data]})")

                # å°è¯•æ•°æ®ç±»å‹è½¬æ¢åŒ¹é…
                original_count = len(filtered_df)

                # æ–¹æ³•1: ç›´æ¥åŒ¹é…
                mask1 = filtered_df[column].isin(values)
                count1 = mask1.sum()

                # æ–¹æ³•2: è½¬æ¢ä¸ºå­—ç¬¦ä¸²ååŒ¹é…
                str_values = [str(v) for v in values]
                mask2 = filtered_df[column].astype(str).isin(str_values)
                count2 = mask2.sum()

                # æ–¹æ³•3: å°è¯•å°†æ•°æ®åˆ—è½¬æ¢ä¸ºæ•°å­—ååŒ¹é…
                try:
                    numeric_column = pd.to_numeric(filtered_df[column], errors='coerce')
                    numeric_values = []
                    for v in values:
                        try:
                            numeric_values.append(float(v))
                        except (ValueError, TypeError):
                            numeric_values.append(v)
                    mask3 = numeric_column.isin(numeric_values)
                    count3 = mask3.sum()
                except:
                    count3 = 0
                    mask3 = pd.Series([False] * len(filtered_df))

                logger.info(f"ğŸ” [åç«¯] åŒ¹é…ç»“æœ - ç›´æ¥åŒ¹é…: {count1}, å­—ç¬¦ä¸²åŒ¹é…: {count2}, æ•°å­—åŒ¹é…: {count3}")

                # é€‰æ‹©åŒ¹é…æ•°é‡æœ€å¤šçš„æ–¹æ³•
                if count3 > 0 and count3 >= max(count1, count2):
                    filtered_df = filtered_df[mask3]
                    logger.info(f"âœ… [åç«¯] ä½¿ç”¨æ•°å­—åŒ¹é…ï¼Œç­›é€‰åæ•°æ®è¡Œæ•°: {len(filtered_df)}")
                elif count2 > 0 and count2 >= count1:
                    filtered_df = filtered_df[mask2]
                    logger.info(f"âœ… [åç«¯] ä½¿ç”¨å­—ç¬¦ä¸²åŒ¹é…ï¼Œç­›é€‰åæ•°æ®è¡Œæ•°: {len(filtered_df)}")
                else:
                    filtered_df = filtered_df[mask1]
                    logger.info(f"âœ… [åç«¯] ä½¿ç”¨ç›´æ¥åŒ¹é…ï¼Œç­›é€‰åæ•°æ®è¡Œæ•°: {len(filtered_df)}")

        else:
            # Optionally, raise an error if the column doesn't exist
            logger.error(f"âŒ [åç«¯] ç­›é€‰åˆ— '{column}' åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨")
            raise HTTPException(status_code=400, detail=f"Filter column '{column}' not found in data.")

    logger.info(f"âœ… [åç«¯] æ•°æ®ç­›é€‰å®Œæˆï¼Œæœ€ç»ˆæ•°æ®è¡Œæ•°: {len(filtered_df)}")

    # Convert NaN to None for JSON compatibility and return as records
    result = filtered_df.where(pd.notnull(filtered_df), None).to_dict(orient='records')
    logger.info(f"ğŸ“¤ [åç«¯] è¿”å›ç­›é€‰ç»“æœ: {len(result)} è¡Œæ•°æ®")

    return result

@app.get("/api/file/{file_id}")
async def get_file_data(file_id: str):
    """
    Retrieves the complete data for a specific file ID.
    """
    logger.info(f"ğŸ“ è¯·æ±‚è·å–æ–‡ä»¶æ•°æ®: {file_id}")

    df = data_storage.get(file_id)
    if df is None:
        logger.error(f"âŒ æ–‡ä»¶IDæœªæ‰¾åˆ°: {file_id}")
        raise HTTPException(status_code=404, detail="File ID not found.")

    # Get headers
    headers = df.columns.tolist()

    # Get all data (convert NaN to None for JSON compatibility)
    all_data = df.where(pd.notnull(df), None).to_dict(orient='records')

    logger.info(f"âœ… æ–‡ä»¶æ•°æ®è·å–æˆåŠŸ: {len(all_data)}è¡Œ Ã— {len(headers)}åˆ—")

    return {
        "file_id": file_id,
        "filename": f"file_{file_id[:8]}.xlsx",  # Generate a filename since we don't store original names
        "headers": headers,
        "preview_data": all_data  # Return all data for editing
    }

@app.post("/api/plot_data")
async def get_plot_data(payload: PlotDataPayload):
    """
    Prepares data for plotting by filtering and extracting x and y axis values.
    """
    df = data_storage.get(payload.file_id)
    if df is None:
        raise HTTPException(status_code=404, detail="File ID not found.")

    # Apply filters first
    filtered_df = df.copy()
    for column, values in payload.filters.items():
        if column in filtered_df.columns:
            if values:  # Ensure there are values to filter by
                filtered_df = filtered_df[filtered_df[column].isin(values)]
        else:
            raise HTTPException(status_code=400, detail=f"Filter column '{column}' not found in data.")

    # Check if x_axis and y_axis columns exist
    if payload.x_axis not in filtered_df.columns:
        raise HTTPException(status_code=400, detail=f"X-axis column '{payload.x_axis}' not found in data.")
    if payload.y_axis not in filtered_df.columns:
        raise HTTPException(status_code=400, detail=f"Y-axis column '{payload.y_axis}' not found in data.")

    # Extract x and y values, removing any NaN values
    x_values = filtered_df[payload.x_axis].dropna().tolist()
    y_values = filtered_df[payload.y_axis].dropna().tolist()

    # Ensure both lists have the same length by taking the minimum length
    min_length = min(len(x_values), len(y_values))
    x_values = x_values[:min_length]
    y_values = y_values[:min_length]

    return {
        "x_values": x_values,
        "y_values": y_values,
        "x_label": payload.x_axis,
        "y_label": payload.y_axis
    }

@app.post("/api/save")
async def save_file_data(payload: SaveFilePayload):
    """
    Saves updated file data back to storage.
    """
    logger.info(f"ğŸ“ è¯·æ±‚ä¿å­˜æ–‡ä»¶æ•°æ®: {payload.file_id}")

    try:
        # éªŒè¯æ•°æ®æ ¼å¼
        if not payload.headers or not isinstance(payload.headers, list):
            raise HTTPException(status_code=400, detail="Invalid headers format")

        if not isinstance(payload.data, list):
            raise HTTPException(status_code=400, detail="Invalid data format")

        # åˆ›å»ºDataFrame
        df = pd.DataFrame(payload.data, columns=payload.headers)

        # æ›´æ–°å­˜å‚¨
        data_storage[payload.file_id] = df

        logger.info(f"âœ… æ–‡ä»¶æ•°æ®ä¿å­˜æˆåŠŸ: {payload.file_id}, æ•°æ®å½¢çŠ¶: {df.shape}")

        return {
            "success": True,
            "message": "File data saved successfully",
            "file_id": payload.file_id,
            "rows": len(payload.data),
            "columns": len(payload.headers)
        }

    except Exception as e:
        logger.error(f"âŒ ä¿å­˜æ–‡ä»¶æ•°æ®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error saving file data: {e}")

@app.get("/api/files")
def list_files():
    """
    Returns a list of all stored files with their metadata.
    """
    logger.info(f"ğŸ“‹ è·å–æ–‡ä»¶åˆ—è¡¨è¯·æ±‚ï¼Œå½“å‰å­˜å‚¨æ–‡ä»¶æ•°: {len(data_storage)}")

    files_info = []
    for file_id, df in data_storage.items():
        # è·å–æ–‡ä»¶å…ƒæ•°æ®
        metadata = file_metadata.get(file_id, {})
        original_filename = metadata.get("original_filename", f"file_{file_id[:8]}.xlsx")
        sheet_name = metadata.get("sheet_name")

        # æ„å»ºæ˜¾ç¤ºæ–‡ä»¶å
        if sheet_name:
            display_filename = f"{original_filename} - {sheet_name}"
        else:
            display_filename = original_filename

        file_info = {
            "file_id": file_id,
            "filename": display_filename,
            "original_filename": original_filename,
            "sheet_name": sheet_name,
            "rows": len(df),
            "columns": len(df.columns),
            "headers": df.columns.tolist(),
            "upload_time": metadata.get("upload_time")
        }
        files_info.append(file_info)

    logger.info(f"âœ… è¿”å› {len(files_info)} ä¸ªæ–‡ä»¶çš„ä¿¡æ¯")
    return {"files": files_info}

@app.delete("/api/file/{file_id}")
async def delete_file_data(file_id: str):
    """
    Deletes a file from storage.
    """
    logger.info(f"ğŸ—‘ï¸ è¯·æ±‚åˆ é™¤æ–‡ä»¶: {file_id}")

    if file_id not in data_storage:
        logger.error(f"âŒ æ–‡ä»¶IDæœªæ‰¾åˆ°: {file_id}")
        raise HTTPException(status_code=404, detail="File ID not found.")

    # åˆ é™¤æ•°æ®å’Œå…ƒæ•°æ®
    del data_storage[file_id]
    if file_id in file_metadata:
        del file_metadata[file_id]

    logger.info(f"âœ… æ–‡ä»¶åˆ é™¤æˆåŠŸ: {file_id}")

    return {
        "success": True,
        "message": "File deleted successfully",
        "file_id": file_id
    }

@app.delete("/api/files/clear")
async def clear_all_files():
    """
    Clears all files from storage.
    """
    logger.info("ğŸ—‘ï¸ è¯·æ±‚æ¸…ç©ºæ‰€æœ‰æ–‡ä»¶")

    file_count = len(data_storage)

    # æ¸…ç©ºæ‰€æœ‰å­˜å‚¨
    data_storage.clear()
    file_metadata.clear()

    logger.info(f"âœ… å·²æ¸…ç©ºæ‰€æœ‰æ–‡ä»¶ï¼Œå…±åˆ é™¤ {file_count} ä¸ªæ–‡ä»¶")

    return {
        "success": True,
        "message": f"All files cleared successfully. Deleted {file_count} files.",
        "deleted_count": file_count
    }

@app.get("/api/unique_values/{file_id}/{column_name}")
async def get_unique_values(file_id: str, column_name: str):
    """
    Returns unique values for a specific column in a file.
    """
    logger.info(f"ğŸ” è¯·æ±‚è·å–å”¯ä¸€å€¼: æ–‡ä»¶ID={file_id}, åˆ—å={column_name}")

    df = data_storage.get(file_id)
    if df is None:
        logger.error(f"âŒ æ–‡ä»¶IDæœªæ‰¾åˆ°: {file_id}")
        raise HTTPException(status_code=404, detail="File ID not found.")

    if column_name not in df.columns:
        logger.error(f"âŒ åˆ—åæœªæ‰¾åˆ°: {column_name}")
        raise HTTPException(status_code=404, detail=f"Column '{column_name}' not found in data.")

    try:
        # è·å–å”¯ä¸€å€¼ï¼Œæ’é™¤NaN
        unique_values = df[column_name].dropna().unique().tolist()
        logger.info(f"âœ… è·å–åˆ° {len(unique_values)} ä¸ªå”¯ä¸€å€¼")

        return {
            "values": unique_values,
            "count": len(unique_values),
            "column": column_name
        }

    except Exception as e:
        logger.error(f"âŒ è·å–å”¯ä¸€å€¼å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting unique values: {e}")

@app.post("/api/predict")
async def generate_prediction(payload: PredictionPayload):
    """
    ä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•ç”Ÿæˆè¶‹åŠ¿é¢„æµ‹
    """
    logger.info(f"ğŸ¤– [é¢„æµ‹] å¼€å§‹é¢„æµ‹ï¼Œæ–‡ä»¶ID: {payload.file_id}, ç®—æ³•: {payload.method}")

    df = data_storage.get(payload.file_id)
    if df is None:
        raise HTTPException(status_code=404, detail="File ID not found.")

    try:
        # åº”ç”¨ç­›é€‰æ¡ä»¶
        filtered_df = df.copy()
        logger.info(f"ğŸ“Š [é¢„æµ‹] åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
        logger.info(f"ğŸ” [é¢„æµ‹] ç­›é€‰æ¡ä»¶: {payload.filters}")

        for column, values in payload.filters.items():
            if column in filtered_df.columns and values:
                before_count = len(filtered_df)
                filtered_df = filtered_df[filtered_df[column].isin(values)]
                after_count = len(filtered_df)
                logger.info(f"ğŸ” [é¢„æµ‹] æŒ‰åˆ— '{column}' ç­›é€‰ {values}: {before_count} -> {after_count} è¡Œ")

        logger.info(f"ğŸ“Š [é¢„æµ‹] ç­›é€‰åæ•°æ®å½¢çŠ¶: {filtered_df.shape}")

        # æ£€æŸ¥è½´åˆ—æ˜¯å¦å­˜åœ¨
        if payload.x_axis not in filtered_df.columns:
            raise HTTPException(status_code=400, detail=f"X-axis column '{payload.x_axis}' not found.")
        if payload.y_axis not in filtered_df.columns:
            raise HTTPException(status_code=400, detail=f"Y-axis column '{payload.y_axis}' not found.")

        # æå–å¹¶æ¸…ç†æ•°æ®
        data_clean = filtered_df[[payload.x_axis, payload.y_axis]].dropna()
        logger.info(f"ğŸ“Š [é¢„æµ‹] æ¸…ç†åæ•°æ®ç‚¹æ•°: {len(data_clean)}")

        if len(data_clean) < 3:
            logger.error(f"âŒ [é¢„æµ‹] æ•°æ®ç‚¹ä¸è¶³: {len(data_clean)} < 3")
            raise HTTPException(status_code=400, detail="Insufficient data points for prediction (minimum 3 required).")

        X = data_clean[payload.x_axis].values.reshape(-1, 1)
        y = data_clean[payload.y_axis].values

        # æ ¹æ®ç®—æ³•ç±»å‹è¿›è¡Œé¢„æµ‹
        prediction_result = await perform_ml_prediction(X, y, payload.method, payload.steps)

        logger.info(f"âœ… [é¢„æµ‹] é¢„æµ‹å®Œæˆï¼Œç®—æ³•: {payload.method}, é¢„æµ‹æ­¥æ•°: {payload.steps}")
        return prediction_result

    except Exception as e:
        logger.error(f"âŒ [é¢„æµ‹] é¢„æµ‹å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Prediction error: {e}")

@app.post("/api/predict_direct")
async def generate_direct_prediction(payload: DirectPredictionPayload):
    """
    ç›´æ¥ä½¿ç”¨æä¾›çš„xå’Œyæ•°æ®è¿›è¡Œæœºå™¨å­¦ä¹ é¢„æµ‹
    """
    logger.info(f"ğŸ¤– [ç›´æ¥é¢„æµ‹] å¼€å§‹é¢„æµ‹ï¼Œç®—æ³•: {payload.method}, æ•°æ®ç‚¹æ•°: {len(payload.x_values)}")

    try:
        # éªŒè¯æ•°æ®
        if len(payload.x_values) != len(payload.y_values):
            raise HTTPException(status_code=400, detail="X and Y values must have the same length.")

        if len(payload.x_values) < 3:
            raise HTTPException(status_code=400, detail="Insufficient data points for prediction (minimum 3 required).")

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        import numpy as np
        X = np.array(payload.x_values).reshape(-1, 1)
        y = np.array(payload.y_values)

        # æ‰§è¡Œé¢„æµ‹
        prediction_result = await perform_ml_prediction(X, y, payload.method, payload.steps)

        logger.info(f"âœ… [ç›´æ¥é¢„æµ‹] é¢„æµ‹å®Œæˆï¼Œç®—æ³•: {payload.method}, é¢„æµ‹æ­¥æ•°: {payload.steps}")
        return prediction_result

    except Exception as e:
        logger.error(f"âŒ [ç›´æ¥é¢„æµ‹] é¢„æµ‹å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Direct prediction error: {e}")

async def perform_ml_prediction(X, y, method: str, steps: int) -> PredictionResult:
    """
    æ‰§è¡Œæœºå™¨å­¦ä¹ é¢„æµ‹
    """
    logger.info(f"ğŸ”¬ [ML] å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œç®—æ³•: {method}, æ•°æ®ç‚¹æ•°: {len(X)}")

    # å‡†å¤‡é¢„æµ‹çš„Xå€¼
    last_x = X[-1, 0]
    step_size = X[-1, 0] - X[-2, 0] if len(X) > 1 else 1.0
    future_x = np.array([last_x + step_size * (i + 1) for i in range(steps)]).reshape(-1, 1)

    model_info = {}
    metrics = {}

    try:
        if method == 'linear':
            # çº¿æ€§å›å½’
            model = LinearRegression()
            model.fit(X, y)
            y_pred_train = model.predict(X)
            y_pred_future = model.predict(future_x)

            model_info = {
                'algorithm': 'çº¿æ€§å›å½’',
                'coefficient': float(model.coef_[0]),
                'intercept': float(model.intercept_)
            }

        elif method == 'polynomial':
            # å¤šé¡¹å¼å›å½’
            degree = min(3, len(X) - 1)  # é¿å…è¿‡æ‹Ÿåˆ
            poly_features = PolynomialFeatures(degree=degree)
            X_poly = poly_features.fit_transform(X)
            future_x_poly = poly_features.transform(future_x)

            model = LinearRegression()
            model.fit(X_poly, y)
            y_pred_train = model.predict(X_poly)
            y_pred_future = model.predict(future_x_poly)

            model_info = {
                'algorithm': f'{degree}æ¬¡å¤šé¡¹å¼å›å½’',
                'degree': degree,
                'features': int(X_poly.shape[1])
            }

        elif method == 'svr':
            # æ”¯æŒå‘é‡æœºå›å½’
            model = SVR(kernel='rbf', C=1.0, gamma='scale')
            model.fit(X, y)
            y_pred_train = model.predict(X)
            y_pred_future = model.predict(future_x)

            model_info = {
                'algorithm': 'æ”¯æŒå‘é‡æœºå›å½’',
                'kernel': 'RBF',
                'support_vectors': int(model.n_support_[0]) if hasattr(model, 'n_support_') else 0
            }

        elif method == 'randomforest':
            # éšæœºæ£®æ—å›å½’
            model = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)
            model.fit(X, y)
            y_pred_train = model.predict(X)
            y_pred_future = model.predict(future_x)

            model_info = {
                'algorithm': 'éšæœºæ£®æ—å›å½’',
                'n_estimators': 100,
                'feature_importance': float(model.feature_importances_[0])
            }

        elif method == 'neuralnetwork':
            # ç¥ç»ç½‘ç»œå›å½’
            model = MLPRegressor(hidden_layer_sizes=(50, 25), max_iter=1000, random_state=42, alpha=0.01)
            model.fit(X, y)
            y_pred_train = model.predict(X)
            y_pred_future = model.predict(future_x)

            model_info = {
                'algorithm': 'ç¥ç»ç½‘ç»œå›å½’',
                'hidden_layers': [50, 25],
                'iterations': int(model.n_iter_)
            }

        elif method == 'xgboost':
            # XGBoostå›å½’ï¼ˆä½¿ç”¨éšæœºæ£®æ—ä½œä¸ºæ›¿ä»£ï¼‰
            model = RandomForestRegressor(n_estimators=200, random_state=42, max_depth=6)
            model.fit(X, y)
            y_pred_train = model.predict(X)
            y_pred_future = model.predict(future_x)

            model_info = {
                'algorithm': 'XGBoostå›å½’ (RandomForestå®ç°)',
                'n_estimators': 200,
                'max_depth': 6
            }

        elif method == 'lstm':
            # LSTMæ—¶é—´åºåˆ—ï¼ˆä½¿ç”¨å¤šé¡¹å¼å›å½’ä½œä¸ºç®€åŒ–å®ç°ï¼‰
            degree = min(2, len(X) - 1)
            poly_features = PolynomialFeatures(degree=degree)
            X_poly = poly_features.fit_transform(X)
            future_x_poly = poly_features.transform(future_x)

            model = LinearRegression()
            model.fit(X_poly, y)
            y_pred_train = model.predict(X_poly)
            y_pred_future = model.predict(future_x_poly)

            model_info = {
                'algorithm': 'LSTMæ—¶é—´åºåˆ— (å¤šé¡¹å¼å®ç°)',
                'sequence_length': min(10, len(X)),
                'degree': degree
            }

        else:
            raise ValueError(f"Unsupported prediction method: {method}")

        # è®¡ç®—æ¨¡å‹è¯„ä¼°æŒ‡æ ‡
        mse = float(mean_squared_error(y, y_pred_train))
        r2 = float(r2_score(y, y_pred_train))
        rmse = float(np.sqrt(mse))

        metrics = {
            'mse': mse,
            'rmse': rmse,
            'r2_score': r2,
            'training_points': len(X)
        }

        logger.info(f"âœ… [ML] æ¨¡å‹è®­ç»ƒå®Œæˆï¼ŒRÂ²: {r2:.4f}, RMSE: {rmse:.4f}")

        return PredictionResult(
            x_values=future_x.flatten().tolist(),
            y_values=y_pred_future.tolist(),
            method=method,
            steps=steps,
            metrics=metrics,
            model_info=model_info
        )

    except Exception as e:
        logger.error(f"âŒ [ML] æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Model training failed: {e}")

if __name__ == "__main__":
    import uvicorn
    import argparse

    parser = argparse.ArgumentParser(description='DaPlot Backend Server')
    parser.add_argument('--port', type=int, default=8001, help='Port to run the server on (default: 8001)')
    args = parser.parse_args()

    logger.info(f"ğŸš€ å¯åŠ¨FastAPIæœåŠ¡å™¨ï¼Œç«¯å£: {args.port}...")
    uvicorn.run(app, host="0.0.0.0", port=args.port, log_level="info")
